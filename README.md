# Dialectic LLM: Tesis‚ÄìAnt√≠tesis‚ÄìS√≠ntesis

Sistema de razonamiento dial√©ctico para evaluar su eficacia en la mejora de la precisi√≥n de LLMs en benchmarks de razonamiento, centr√°ndose en el proceso **Tesis ‚Üí Ant√≠tesis ‚Üí S√≠ntesis (T-A-S)**.

## üéØ Descripci√≥n General del Proyecto

Este proyecto explora la hip√≥tesis de que un framework de razonamiento dial√©ctico puede mejorar la capacidad de los Large Language Models (LLMs) para resolver problemas complejos. El framework opera en tres fases:
- **Thesis**: Genera una soluci√≥n inicial (exploraci√≥n creativa).
- **Antithesis**: Critica y cuestiona la soluci√≥n propuesta.
- **Synthesis**: Unifica ambas perspectivas en una respuesta mejorada y m√°s robusta.

**Objetivo**: Evaluar emp√≠ricamente si este m√©todo dial√©ctico ofrece una mejora estad√≠sticamente significativa en la precisi√≥n (ŒîAcc) y/o una mayor robustez en la resoluci√≥n de problemas, considerando siempre la eficiencia en el consumo de tokens (costo de generaci√≥n).

### Criterios de √âxito M√≠nimo (Proyecto)

Para considerar el m√©todo dial√©ctico exitoso, se esperaba que:
- **En al menos un dataset:** ŒîAcc ‚â• +5pp Y costo ‚â§2.5√ó tokens de generaci√≥n.
- **En el otro dataset:** ŒîAcc ‚â• 0pp (no-regresi√≥n) Y `invalid/format` ‚â§ baseline + 2pp.

## üìä Estado Actual y Hallazgos Clave

### GSM8K (Problemas de razonamiento matem√°tico estructurado)

*   **Evaluaci√≥n:** Realizada con 50 problemas (Sprint 2).
*   **Resultados:**
    *   **Baseline:** Alta precisi√≥n (e.g., 98% accuracy).
    *   **T-A-S (k=1):** Mostr√≥ una **disminuci√≥n** de precisi√≥n (ej. -2pp accuracy) con un **incremento significativo en el costo** (ej. 16√ó m√°s tokens).
    *   **T-A-S+MAMV (k=1):** No mejor√≥ la precisi√≥n (0pp) y fue a√∫n **m√°s costoso** (ej. 47√ó m√°s tokens).
*   **Conclusi√≥n:** El m√©todo dial√©ctico T-A-S **no aport√≥ beneficios en precisi√≥n** para el dataset GSM8K, un benchmark con respuestas num√©ricas y directas. La narrativa honesta es que, para este tipo de problemas, el costo computacional no se justifica por una mejora en el rendimiento.

### TruthfulQA (Preguntas enga√±osas/ambiguas que requieren pensamiento cr√≠tico)

*   **Evaluaci√≥n:** Realizada con 50 problemas (Sprint 3).
*   **Resultados:** Ambos m√©todos (Baseline y T-A-S) obtuvieron **0% de precisi√≥n** bajo una evaluaci√≥n de `exact-match` estricta. T-A-S incurri√≥ en **32√ó m√°s costo**.
*   **Hallazgo:** La baja precisi√≥n se debe principalmente a la **incompatibilidad de la m√©trica `exact-match`** con las respuestas verbose y de meta-razonamiento generadas por los LLMs (especialmente T-A-S), no a una falla inherente de los m√©todos. El contenido sem√°ntico de las respuestas a menudo es correcto, pero el formato no coincide con la respuesta esperada.
*   **Conclusi√≥n:** TruthfulQA, bajo la m√©trica actual, **no es un dataset √∫til para evaluar mejoras de precisi√≥n** de nuestro m√©todo dial√©ctico en este contexto.

### Contexto del M√©todo T-A-S

Es crucial entender que el m√©todo T-A-S original (Abdali et al., 2025) fue dise√±ado para **generaci√≥n de ideas y creatividad**, no para optimizar la precisi√≥n en benchmarks de razonamiento como GSM8K o TruthfulQA. Nuestro proyecto ha sido el **primero en evaluar emp√≠ricamente T-A-S en estos benchmarks**, demostrando que su valor reside en la generaci√≥n de razonamiento detallado m√°s que en la mejora de una m√©trica de precisi√≥n estricta.

## üöÄ Instalaci√≥n

### Requisitos
- Python 3.13+
- `uv` (gestor de paquetes)

### Setup

```bash
# 1. Clonar repositorio
git clone https://github.com/This-au-Chocolat/dialectic-llm.git
cd dialectic-llm

# 2. Instalar dependencias con uv
uv sync

# 3. Configurar variables de entorno
cp .env.example .env
# Editar .env con tu OPENAI_API_KEY o DEEPSEEK_API_KEY (requerido para los runs)
```

## üèóÔ∏è Estructura del Proyecto

```
dialectic-llm/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ flows/              # Prefect flows (baseline, T-A-S)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline.py     # Flow baseline (single-call)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tas.py          # Flow T-A-S dial√©ctico (k=1)
‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Utilidades compartidas (data loading, evaluation, logging, etc.)
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ model.yaml          # Configuraci√≥n de modelos
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ tas/                # Templates de prompts (thesis, antithesis, synthesis)
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ events/             # Logs JSONL sanitizados de las ejecuciones
‚îú‚îÄ‚îÄ logs_local/             # Logs JSONL locales con Chain-of-Thought completo (gitignored)
‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îî‚îÄ‚îÄ parquet/            # Archivos Parquet para an√°lisis de resultados
‚îú‚îÄ‚îÄ tests/                  # Tests unitarios del proyecto
‚îî‚îÄ‚îÄ reports/                # Documentaci√≥n y an√°lisis de sprints
```

## üéÆ Uso (Reproducci√≥n de Corridas Clave)

Este proyecto se enfoca en la evaluaci√≥n del m√©todo T-A-S. Para reproducir las corridas principales:

### 1. Preparar IDs de Problemas Comunes (TruthfulQA)

Para asegurar la comparabilidad estad√≠stica, se utiliza un conjunto fijo de `problem_ids`.

```bash
# (Este archivo ya deber√≠a existir en `data/processed/common_problem_ids.txt`)
# Si no existe, puedes generarlo desde el script de preparaci√≥n de datos,
# asegur√°ndote de usar los mismos 50 IDs de TruthfulQA para todos los runs.
```

### 2. Ejecutar Baseline en TruthfulQA (50 problemas)

Este script ejecuta la l√≠nea base en el dataset TruthfulQA.

```bash
uv run python scripts/run_s3_07_baseline_tqa_50.py
# Resultados en: analytics/parquet/baseline_tqa_50_*.parquet
```

### 3. Ejecutar T-A-S (k=1) en TruthfulQA (50 problemas)

Este script ejecuta el flujo T-A-S dial√©ctico en TruthfulQA.

```bash
uv run python scripts/run_s3_08_tas_tqa_50.py
# Resultados en: analytics/parquet/tas_tqa_50_*.parquet
```

### 4. An√°lisis de KPI y Taxonom√≠a de Errores

Tras ejecutar las corridas, se pueden generar los KPIs y la taxonom√≠a de errores.

```bash
# Ejecutar el an√°lisis de KPIs (si est√° disponible un script actualizado para TQA)
# `scripts/run_s3_13_mcnemar_analysis.py` -> A√∫n enfocado en GSM8K, adaptable para TQA.

# Generar taxonom√≠a de errores (ya realizada con resultados en analytics/parquet/)
# Los resultados de la taxonom√≠a de errores (S3-15) ya est√°n disponibles en:
# - `analytics/parquet/error_taxonomy_labeled.parquet`
# - `analytics/parquet/error_category_counts.json`
```

## ‚ùå Caracter√≠sticas No Exploradas/Eliminadas

Durante el desarrollo, se tomaron decisiones de dise√±o y alcance para mantener el enfoque y la eficiencia del proyecto:

*   **Uso expl√≠cito de "Debate"**: Si bien el m√©todo T-A-S es inherentemente dial√©ctico, la implementaci√≥n de un "corpus de debate" o prompts de debate expl√≠citos (m√°s all√° de T-A-S) se consider√≥ un feature adicional costoso y no esencial para la hip√≥tesis principal, siendo eliminada del plan.
*   **T-A-S con k=2 (m√∫ltiples rondas)**: La ejecuci√≥n de T-A-S con `k=2` (dos rondas de T-A-S) fue eliminada debido a su **alto costo computacional** (ej. 100√ó el baseline) y a que el desempe√±o de `k=1` no justific√≥ una mayor exploraci√≥n.
*   **Re-corridas extensas en GSM8K**: Tras los resultados del Sprint 2, que mostraron que T-A-S no mejor√≥ la precisi√≥n en GSM8K, se decidi√≥ no realizar re-corridas extensas en este dataset para el an√°lisis final, priorizando TruthfulQA.

## üîê Seguridad y Privacidad

### Chain-of-Thought (CoT)
- ‚ö†Ô∏è **NUNCA** se comparten los logs con CoT completo
- CoT solo en `logs_local/` (gitignored)
- Logs compartidos en `logs/events/` est√°n sanitizados

### Sanitizaci√≥n
- Informaci√≥n Personal Identificable (PII) detectada y redactada.
- Prompts y respuestas hasheados (`prompt_hash`, `response_hash`).
- Whitelist estricta de campos permitidos.

### Seguridad de Costos
- L√≠mites de costo por ejecuci√≥n y alertas para evitar exceder el presupuesto.
- Conteo de tokens autom√°tico para monitoreo.

## üß™ Tests

```bash
# Ejecutar todos los tests
uv run pytest tests/

# Tests espec√≠ficos (ejemplo)
uv run pytest tests/test_prompt_utils.py -v

# Con cobertura
uv run pytest tests/ --cov=src --cov-report=html
```

## üë• Equipo

- **This au Chocolat** - Scrum Master + Orchestration
- **Julio de Aquino** - MLE
- **Jos√© Pech** - Data / Evaluaci√≥n
- **Lorena P√©rez** - AI Safety & Compliance
- **Valeria Hern√°ndez** - Tech Writing

## üìù Licencia

[Especificar licencia]

## üîó Referencias

- [GSM8K Dataset](https://github.com/openai/grade-school-math)
- [Prefect Documentation](https://docs.prefect.io/)
- [Paper Original T-A-S (Abdali et al., 2025)](https://arxiv.org/abs/2501.14917)
- [Paper del Proyecto (TBD)]

---

**√öltima Actualizaci√≥n**: 2 de diciembre de 2025
